# SPDX-License-Identifier: MIT
# AdamWPrune Configuration

mainmenu "AdamWPrune v1.0.0 - Neural Network Training Configuration"

menu "General Configuration"

config BATCH_SIZE
	int "Batch size for training"
	default 512
	range 1 2048
	help
	  Batch size for training. Larger batch sizes can speed up training
	  but require more GPU memory. Default is 512.
	  For GPT-2 with large context (1024), smaller batch sizes (8-16)
	  may be needed to fit in memory.

config NUM_EPOCHS
	int "Number of training epochs"
	default 10
	range 1 1000
	help
	  Number of epochs to train the model. Default is 10.

config LEARNING_RATE
	string "Learning rate"
	default "0.001"
	help
	  Initial learning rate for training. Default is 0.001.

	  IMPORTANT - Model-specific behavior:

	  LeNet-5: Uses this value directly for all optimizers
	  - Typical values: 0.001 for Adam-based, 0.01-0.1 for SGD

	  ResNet-18: Auto-scales for Adam optimizers (resnet18/train.py line ~130)
	  - SGD: Uses the configured value directly
	  - Adam/AdamW/AdamWPrune: Multiplies by 0.001
	  - Example: Set to 0.1 to get SGD=0.1, Adam-based=0.0001

config NUM_WORKERS
	int "Number of data loader workers"
	default 16
	range 0 32
	help
	  Number of parallel workers for data loading. Set to 0 to disable
	  multiprocessing. Default is 16.

config DEVICE
	string "Device for training"
	default "cuda"
	help
	  Device to use for training. Can be "cuda" for GPU or "cpu" for CPU.
	  Default is "cuda" which will automatically fall back to CPU if no
	  GPU is available.

config PYTORCH_CUDA_ALLOC_CONF
	string "PyTorch CUDA memory allocator configuration"
	default "expandable_segments:True"
	help
	  Configuration for PyTorch CUDA memory allocator to prevent OOM.

	  Default: "expandable_segments:True"
	    - Enables expandable memory segments
	    - Prevents memory fragmentation
	    - Critical for avoiding OOM with torch.compile()

	  See /data/TheRock/HIP_OOM_MITIGATION.md for details.

	  This is set as an environment variable BEFORE importing torch.

config TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL
	bool "Enable AOTriton experimental features for ROCm"
	default y
	depends on DEVICE = "cuda"
	help
	  Enable experimental AOTriton features for ROCm/HIP backend.
	  Required for flash attention support on AMD GPUs.

	  When enabled, sets TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1
	  before importing torch.

	  This enables memory efficient attention (flash attention) which
	  provides 2-4x memory reduction for attention layers.

config DATA_DIR
	string "Data directory path"
	default "data"
	help
	  Path to the directory where datasets will be stored/loaded.
	  Default is "data" in the project root.

config OUTPUT_DIR
	string "Output directory for results"
	default "results"
	help
	  Directory where training results, models, and metrics will be saved.
	  Default is "results".

config JSON_OUTPUT
	string "JSON metrics output filename"
	default "training_metrics.json"
	help
	  Filename for saving training metrics in JSON format.
	  Default is "training_metrics.json".

endmenu

source "Kconfig.models"
source "Kconfig.optimizers"
source "Kconfig.pruning"
source "Kconfig.ra_mla"

menu "Advanced Options"

config COMPILE_MODEL
	bool "Enable torch.compile() for model optimization"
	default y
	depends on DEVICE = "cuda"
	help
	  Enable PyTorch 2.0+ torch.compile() for faster execution.
	  Only available when using CUDA device.

config MIXED_PRECISION
	bool "Enable mixed precision training (AMP)"
	default y
	depends on DEVICE = "cuda"
	help
	  Enable Automatic Mixed Precision (AMP) training for faster
	  training and reduced memory usage on GPUs.

config GPU_WARMUP
	bool "Enable GPU warmup"
	default y
	depends on DEVICE = "cuda"
	help
	  Perform GPU warmup iterations before training to ensure
	  consistent timing measurements.

config PIN_MEMORY
	bool "Pin memory for data loading"
	default y
	depends on DEVICE = "cuda"
	help
	  Pin memory for faster GPU transfer during data loading.

config GPU_MONITOR
	bool "Enable GPU memory monitoring"
	default n
	depends on DEVICE = "cuda"
	help
	  Monitor GPU memory usage during training and inference.
	  Saves data to JSON files for analysis and visualization.

config PERSISTENT_WORKERS
	bool "Keep data loader workers persistent"
	default y
	depends on NUM_WORKERS > 0
	help
	  Keep data loader workers alive between epochs to reduce overhead.

config PREFETCH_FACTOR
	int "Prefetch factor for data loading"
	default 2
	range 1 10
	depends on NUM_WORKERS > 0
	help
	  Number of batches to prefetch per worker. Default is 2.


config SAVE_CHECKPOINT
	bool "Save model checkpoints"
	default y
	help
	  Save model checkpoints during training.

config CHECKPOINT_INTERVAL
	int "Checkpoint save interval (epochs)"
	default 1
	range 1 100
	depends on SAVE_CHECKPOINT
	help
	  Save checkpoints every N epochs. Default is 1.

endmenu

menu "Experiment Tracking"

comment "Enable any combination of trackers (they can run simultaneously)"
comment "CLI: make defconfig TRACKER=wandb,trackio (or both, none, wandb, trackio)"

# CLI override detection
config TRACKER_SET_BY_CLI
	bool
	output yaml
	default $(shell, scripts/check-cli-set-var.sh TRACKER)

config TRACKER_CLI_VALUE
	string
	default "$(TRACKER)" if TRACKER_SET_BY_CLI
	default ""

config ENABLE_TRACKIO
	bool "Enable Trackio (local tracking)"
	default y if TRACKER_SET_BY_CLI && $(shell, scripts/check-tracker-enabled.sh trackio $(TRACKER)) = "y"
	default n
	help
	  Use Trackio for lightweight, local-first experiment tracking.
	  Stores data in local SQLite database with web dashboard.
	  Install with: pip install trackio
	  Can be used together with WandB for comparison.
	  CLI: make defconfig TRACKER=trackio
	       make defconfig TRACKER=wandb,trackio

config ENABLE_WANDB
	bool "Enable Weights & Biases (cloud tracking)"
	default y if TRACKER_SET_BY_CLI && $(shell, scripts/check-tracker-enabled.sh wandb $(TRACKER)) = "y"
	default n
	help
	  Use Weights & Biases for cloud-based experiment tracking.
	  Provides advanced features like sweeps and team collaboration.
	  Install with: pip install wandb
	  Can be used together with Trackio for comparison.
	  CLI: make defconfig TRACKER=wandb
	       make defconfig TRACKER=wandb,trackio

config TRACKER_PROJECT
	string "Project name for experiment tracking"
	default $(shell, scripts/generate-project-name.sh $(MODEL) tracking)
	depends on ENABLE_TRACKIO || ENABLE_WANDB
	help
	  Project name to organize experiments in the tracking system.
	  Auto-generated as: {model}-{checksum}
	  where model is the selected model and checksum is derived from
	  hostname, IP, and current directory for uniqueness.
	  Used by both Trackio and WandB when enabled.

config TRACKER_RUN_NAME
	string "Run name for experiment tracking"
	default ""
	depends on ENABLE_TRACKIO || ENABLE_WANDB
	help
	  Optional custom name for this training run.
	  If empty, will be auto-generated with format:
	  {model}_{optimizer}_{timestamp}

config WANDB_PROJECT
	string "WandB-specific project name (overrides TRACKER_PROJECT)"
	default ""
	depends on ENABLE_WANDB
	help
	  Optional WandB-specific project name.
	  If empty, uses TRACKER_PROJECT.
	  Will be used for wandb.init(project=...).

config WANDB_ENTITY
	string "WandB entity (team/username)"
	default ""
	depends on ENABLE_WANDB
	help
	  WandB entity (team or username) for experiment tracking.
	  Leave empty to use default entity.

config WANDB_OFFLINE
	bool "Run WandB in offline mode"
	default n
	depends on ENABLE_WANDB
	help
	  Run WandB in offline mode without requiring login.
	  Runs can be synced later with 'wandb sync'.

config TRACKIO_PORT
	int "Port for Trackio dashboard"
	default 7860
	depends on ENABLE_TRACKIO
	help
	  Port number for the Trackio web dashboard.
	  Access dashboard with: trackio show --port PORT

endmenu

menu "Test Matrix"

config TEST_RESULTS_DIR
	string "Test results directory for graph generation"
	default ""
	help
	  Specify the test matrix results directory to use for generating graphs.
	  Leave empty to use the most recent test_matrix_results_* directory.
	  Example: test_matrix_results_20250826_181029

config AUTO_GENERATE_GRAPHS
	bool "Automatically generate graphs after test matrix"
	default y
	help
	  Automatically generate comparison graphs for each optimizer after
	  completing the test matrix run.

config PARALLEL_JOBS
	int "Number of parallel training jobs"
	default 1
	range 1 50
	help
	  Number of training jobs to run in parallel. Each job uses minimal
	  GPU memory (~50-100MB for LeNet-5), so multiple jobs can run
	  simultaneously on high-memory GPUs. Recommended: 1-4 for <16GB GPU,
	  8-20 for 24-48GB GPU, up to 50 for very large GPUs.

config MAX_GPU_MEMORY_PERCENT
	int "Maximum GPU memory usage percentage"
	default 90
	range 50 95
	depends on PARALLEL_JOBS > 1
	help
	  Maximum percentage of GPU memory to use for parallel training.
	  The system will automatically limit concurrent jobs to stay within
	  this threshold. Default is 90%.

config PARALLEL_BATCH_SIZE
	int "Batch size for parallel jobs"
	default 256
	range 32 1024
	depends on PARALLEL_JOBS > 1
	help
	  Batch size to use when running parallel jobs. Smaller batch sizes
	  reduce memory usage per job, allowing more parallel execution.
	  Default is 256 (half of normal batch size).

endmenu

menu "Debugging"

config DEBUG
	bool "Enable debug mode"
	default n
	help
	  Enable debug mode with verbose logging and additional checks.

config VERBOSE
	bool "Enable verbose output"
	default y
	help
	  Enable verbose output during training.

config LOG_LEVEL
	string "Logging level"
	default "INFO"
	help
	  Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).
	  Default is INFO.

config PROFILE
	bool "Enable profiling"
	default n
	help
	  Enable performance profiling during training.

config INFERENCE_TEST
	bool "Test inference after training"
	default n
	help
	  Run inference testing and memory measurement after training completes.
	  This will measure actual GPU memory usage during inference with various
	  batch sizes and compare memory usage across different models.

config INFERENCE_BATCH_SIZES
	string "Batch sizes for inference testing"
	default "1,32,128,256"
	depends on INFERENCE_TEST
	help
	  Comma-separated list of batch sizes to test during inference.
	  Default is "1,32,128,256".

config SAVE_CHECKPOINT
	bool "Save model checkpoint after training"
	default y
	help
	  Save the trained model checkpoint after training completes.
	  Required for inference testing. Models can be large (180MB+ for ResNet-18).

endmenu
