# SPDX-License-Identifier: MIT
# Test AdamWPrune bitter5-9 variants inspired by ChatGPT alternatives
# Tests new approaches: movement-to-zero, coherence, second-moment, bias-corrected, hybrid

# Model Selection - Use GPT-2
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_MODE_MULTIPLE=n
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_SELECT_LENET5=n
CONFIG_MODEL_SELECT_RESNET18=n
CONFIG_MODEL_SELECT_RESNET50=n
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"
CONFIG_TEST_MODELS="gpt2"

# GPT-2 Model Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=4
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_FLASH_ATTENTION=y
CONFIG_GPT2_COMPILE=y

# Enable new bitter variants (5-9) for testing
# CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER0 is not set
# CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER1 is not set
# CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER2 is not set
# CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER3 is not set
# CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER4 is not set
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER5=y  # Movement-to-zero
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER6=y  # Coherence-weighted
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER7=y  # Second-moment based
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER8=y  # Bias-corrected
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER9=y  # Hybrid signals

# Training Configuration
CONFIG_BATCH_SIZE=32
CONFIG_MAX_ITERS=10000  # Will be auto-adjusted to 13000 for bitter5/6/8/9
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_DEVICE="cuda"
CONFIG_DTYPE="bfloat16"
CONFIG_LOG_INTERVAL=10

# Optimizer Configuration - Test AdamWPrune only
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWPRUNE=y
CONFIG_OPTIMIZER="adamwprune"

# AdamWPrune configuration
CONFIG_ADAMWPRUNE_BASE_ADAMWSPAM=y
CONFIG_ADAMWPRUNE_ENABLE_PRUNING=y
CONFIG_ADAMWPRUNE_TARGET_SPARSITY="0.5"
CONFIG_ADAMWPRUNE_BETA1="0.9"
CONFIG_ADAMWPRUNE_BETA2="0.95"
CONFIG_ADAMWPRUNE_WEIGHT_DECAY="0.1"
CONFIG_ADAMWPRUNE_WARMUP_STEPS=100
CONFIG_ADAMWPRUNE_FREQUENCY=50
CONFIG_ADAMWPRUNE_AMSGRAD=y

# SPAM settings (for base optimizer)
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning Configuration - Use state pruning only
CONFIG_PRUNING_MODE_SINGLE=y
CONFIG_PRUNING_SELECT_STATE=y
CONFIG_PRUNING_METHOD="state"

# Test only 50% sparsity for consistency
CONFIG_TARGET_SPARSITY="0.5"
CONFIG_SPARSITY_ENABLE_50=y
CONFIG_SPARSITY_ENABLE_70=n
CONFIG_SPARSITY_ENABLE_90=n
CONFIG_TEST_SPARSITY_50=y
CONFIG_PRUNING_WARMUP=100
CONFIG_PRUNING_FREQUENCY=50
CONFIG_PRUNING_LOG_SPARSITY=y

# Output Configuration
CONFIG_OUTPUT_DIR="bitter5_9_comparison"
CONFIG_JSON_OUTPUT="bitter5_9_metrics.json"
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=5000

# Tracking Configuration
CONFIG_TRACKER="wandb,trackio"

# Resource Configuration
CONFIG_NUM_WORKERS=16
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=2
CONFIG_COMPILE_MODEL=y
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y

# Verbose output for analysis
CONFIG_VERBOSE=y
CONFIG_LOG_LEVEL="INFO"
CONFIG_DEBUG=n
CONFIG_PROFILE=n

# Expected outcomes based on design:
# bitter5: Movement-to-zero - tracks weights being pushed to zero (~44-46 ppl)
# bitter6: Coherence-weighted - penalizes oscillatory gradients (~42-44 ppl)
# bitter7: Second-moment - uses variance signal (~45-47 ppl)
# bitter8: Bias-corrected - removes Adam's early bias (~42-43 ppl)
# bitter9: Hybrid - combines multiple signals (~43-45 ppl)