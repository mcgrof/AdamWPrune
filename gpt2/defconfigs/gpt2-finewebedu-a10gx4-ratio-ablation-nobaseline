# RATIO Framework: Ablation Steps 2-14 (Skip Baseline Steps 0-1)
# Tests golden ratio (1:2.5), MLP mechanisms, RA, MLA, and structure-aware optimization
#
# Hardware: 4Ã— NVIDIA A10G (24GB each) with DDP
# Based on: gpt2-ratio-ablation with fixes for OOM and tensor mismatch
# Skips: Steps 0-1 which already passed in previous test run
#
# To run: make defconfig-gpt2-ratio-ablation-no-baseline && make

# Enable GPT-2 model
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration - Matched to gpt2-ratio-ablation
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=4
CONFIG_GPT2_MAX_ITERS=10400
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=n
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Training parameters - Reduced batch_size for memory safety
# Original: batch_size=16 caused OOM on A10G 24GB with entropy computation
# New: batch_size=8 with 2x gradient accumulation for same effective batch
CONFIG_BATCH_SIZE=8
CONFIG_NUM_EPOCHS=10
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=16
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="./gpt2/data"
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Optimizer - AdamWSPAM (for all steps except step 1 which uses pruning)
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled (will be enabled per-step for steps 1 and 14)
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA+MLA Configuration for RATIO ablation
CONFIG_ENABLE_RA_MLA=y
CONFIG_RA_MLA_LATENT_DIM=128
CONFIG_RA_MLA_RA_WINDOW=64
CONFIG_RA_MLA_RA_ALPHA="0.3"
CONFIG_RA_MLA_PER_HEAD_Q_LATENT=y
CONFIG_RA_MLA_PER_HEAD_V_UP=y
CONFIG_RA_MLA_USE_FLASH=n
# CRITICAL FIX: Disable metrics logging to prevent OOM during entropy computation
# Entropy computation creates large intermediate tensors (768MB) that cause OOM with batch_size=16
# Tests 3, 5, 6+ failed with OOM in _compute_entropy() due to this
CONFIG_RA_MLA_LOG_METRICS=n
CONFIG_RA_MLA_CACHE_Q_WINDOW=y
CONFIG_RA_MLA_USE_ROPE=n

# Enable ablation study mode, skipping baseline steps 0-1
# Steps 0-1 already passed in test_matrix_results_20251102_032010
# Running steps 2-14: golden ratio, MLP mechanisms, RA, MLA combinations
CONFIG_RA_MLA_ABLATION_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="2,3,4,5,6,7,8,9,10,11,12,13,14"

# Enable test matrix mode
CONFIG_TEST_MATRIX_MODE=y

# Experiment Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=n
CONFIG_TRACKER_PROJECT="gpt2-ratio-ablation-no-baseline"

# Advanced options - torch.compile disabled for RA+MLA (inductor bug)
CONFIG_COMPILE_MODEL=n
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000

# Memory Optimizations - CRITICAL for A10G GPUs preventing fragmentation
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Test execution
CONFIG_TEST_PARALLEL_JOBS=1
CONFIG_OUTPUT_DIR="test_matrix_results_ratio_ablation_no_baseline"

# Notes on step-specific configuration:
# - Step 2: mlp_dim=3840 for golden ratio 1:2.5
# - Step 3: mlp_dim=3264 (85%), gating 15% - FIXED tensor mismatch issue
# - Step 4: mlp_dim=3072 (80%), gating+cross-token 20%
# - Step 5: ra_alpha=0.3, mlp_dim=3072
# - Step 6: ra_alpha=0.3, mlp_dim=3840
# - Step 7: ra_alpha=0.3, mlp_dim=3072, mechanisms 20%
# - Step 8: MLA enabled, mlp_dim=3072 (ratio 1:3.0)
# - Step 9: MLA enabled, mlp_dim=2560 (ratio 1:2.5)
# - Step 10: MLA enabled, mlp_dim=2048 (80%), mechanisms 20%
# - Step 11: MLA + RA, mlp_dim=2560
# - Step 12: MLA + RA, mlp_dim=2048, mechanisms 20%
# - Step 13: MLA, mlp_dim=2048, AdamWStructure (TODO)
# - Step 14: MLA, mlp_dim=2048, ratio-preserving pruning (TODO)
#
# Fixes applied in this version:
# 1. CONFIG_RA_MLA_LOG_METRICS=n - Disables entropy computation (OOM fix)
# 2. CONFIG_BATCH_SIZE=8 - Reduced from 16 for additional memory safety
# 3. ra_mla_gpt2.py handles MLP dimension mismatch when copying weights
# 4. ra_mla_gpt2.py has memory-efficient entropy computation (when enabled)
