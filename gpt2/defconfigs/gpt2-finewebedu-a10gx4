#
# GPT-2 Configuration for AWS g5.12xlarge (4x NVIDIA A10G)
# FineWebEdu dataset, optimized for 4x24GB GPUs
#

# Model Selection
CONFIG_MODEL="gpt2"
CONFIG_GPT2_MODEL_SIZE="124M"
CONFIG_GPT2_COMPILE_MODEL=y

# Multi-GPU Configuration
CONFIG_GPT2_USE_DDP=y
CONFIG_GPT2_NUM_GPUS=4
CONFIG_GPT2_DDP_BACKEND="nccl"
CONFIG_GPT2_DDP_FIND_UNUSED_PARAMS=n

# Training Parameters
CONFIG_GPT2_BATCH_SIZE=24
CONFIG_GPT2_GRADIENT_ACCUMULATION=4
CONFIG_GPT2_MAX_ITERS=600000
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10

# Learning Rate Schedule
CONFIG_GPT2_LEARNING_RATE="6e-4"
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_WEIGHT_DECAY="0.1"
CONFIG_GPT2_GRAD_CLIP="1.0"

# Adam Optimizer Settings
CONFIG_GPT2_ADAM_BETA1="0.9"
CONFIG_GPT2_ADAM_BETA2="0.95"
CONFIG_GPT2_ADAM_EPSILON="1e-8"

# Dataset Configuration
CONFIG_GPT2_DATA_DIR="./data"
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024

# Data Loading Optimization
CONFIG_GPT2_NUM_WORKERS=12
CONFIG_GPT2_PIN_MEMORY=y
CONFIG_GPT2_PERSISTENT_WORKERS=y
CONFIG_GPT2_PREFETCH_FACTOR=4

# Mixed Precision Training
CONFIG_GPT2_MIXED_PRECISION=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Performance Optimizations
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPT2_GPU_WARMUP=y
CONFIG_GPT2_COMPILE=y

# Checkpointing
CONFIG_GPT2_SAVE_CHECKPOINT=y
CONFIG_GPT2_CHECKPOINT_INTERVAL=1000
CONFIG_GPT2_OUTPUT_DIR="./checkpoints"

# Memory Management
CONFIG_GPT2_GRADIENT_CHECKPOINTING=n
CONFIG_GPT2_EMPTY_CACHE_FREQ=100

# Experiment Tracking
CONFIG_GPT2_TRACKER="none"
CONFIG_GPT2_TRACKER_PROJECT="gpt2-a10gx4"

# Distributed Training Settings
CONFIG_GPT2_MASTER_ADDR="localhost"
CONFIG_GPT2_MASTER_PORT="12355"