# SPDX-License-Identifier: MIT
# Test AdamWPrune bitter lesson variants against AdamWSPAM with magnitude pruning
# All tests target 50% sparsity on GPT-2 with finewebedu dataset
#
# Tests to run:
# 1. AdamWSPAM + magnitude pruning @ 50% (baseline)
# 2. AdamWPrune bitter0 (original hybrid) + state pruning @ 50%
# 3. AdamWPrune bitter1 (pure magnitude) + state pruning @ 50%
# 4. AdamWPrune bitter2 (scale-aware magnitude, +21% iters) + state pruning @ 50%

# Model Selection - Use GPT-2
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_MODE_MULTIPLE=n
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_SELECT_LENET5=n
CONFIG_MODEL_SELECT_RESNET18=n
CONFIG_MODEL_SELECT_RESNET50=n
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"
CONFIG_TEST_MODELS="gpt2"

# GPT-2 Model Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=4
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_FLASH_ATTENTION=y
CONFIG_GPT2_COMPILE=y

# Enable bitter lesson variants
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER0=y
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER1=y
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER2=y

# Training Configuration
CONFIG_BATCH_SIZE=32
CONFIG_MAX_ITERS=10000
CONFIG_NUM_EPOCHS=n  # GPT-2 uses iterations, not epochs
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_DEVICE="cuda"
CONFIG_DTYPE="bfloat16"
CONFIG_LOG_INTERVAL=10

# Enable test matrix mode
CONFIG_TEST_MATRIX_MODE=y

# Optimizer Configuration - Test AdamWSPAM and AdamWPrune
CONFIG_OPTIMIZER_MODE_MULTIPLE=y
CONFIG_OPTIMIZER_ENABLE_ADAMWSPAM=y
CONFIG_OPTIMIZER_ENABLE_ADAMWPRUNE=y
CONFIG_TEST_OPTIMIZER_ENABLED_ADAMWSPAM=y
CONFIG_TEST_OPTIMIZER_ENABLED_ADAMWPRUNE=y
# Disable other optimizers
CONFIG_OPTIMIZER_ENABLE_SGD=n
CONFIG_OPTIMIZER_ENABLE_ADAM=n
CONFIG_OPTIMIZER_ENABLE_ADAMW=n
CONFIG_OPTIMIZER_ENABLE_ADAMWADV=n

# AdamWPrune configuration
CONFIG_ADAMWPRUNE_BASE_ADAMWSPAM=y
CONFIG_ADAMWPRUNE_ENABLE_PRUNING=y
CONFIG_ADAMWPRUNE_TARGET_SPARSITY="0.5"
CONFIG_ADAMWPRUNE_BETA1="0.9"
CONFIG_ADAMWPRUNE_BETA2="0.95"
CONFIG_ADAMWPRUNE_WEIGHT_DECAY="0.1"
CONFIG_ADAMWPRUNE_WARMUP_STEPS=100
CONFIG_ADAMWPRUNE_FREQUENCY=50
CONFIG_ADAMWPRUNE_RAMP_END_STEP=10000

# Test all bitter variants
CONFIG_TEST_ADAMWPRUNE_VARIANTS="bitter0,bitter1,bitter2"

# SPAM settings (for both optimizers)
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning Configuration
CONFIG_PRUNING_MODE_MULTIPLE=y
CONFIG_PRUNING_ENABLE_MAGNITUDE=y  # For AdamWSPAM baseline
CONFIG_PRUNING_ENABLE_STATE=y      # For AdamWPrune variants
CONFIG_PRUNING_ENABLE_MOVEMENT=n
CONFIG_PRUNING_ENABLE_NONE=n

# Test matrix: AdamWSPAM uses magnitude, AdamWPrune uses state
CONFIG_TEST_PRUNING_MAGNITUDE=y    # AdamWSPAM
CONFIG_TEST_PRUNING_STATE=y        # AdamWPrune
CONFIG_TEST_PRUNING_MOVEMENT=n
CONFIG_TEST_PRUNING_NONE=n

# Only test 50% sparsity
CONFIG_TARGET_SPARSITY="0.5"
CONFIG_SPARSITY_ENABLE_50=y
CONFIG_SPARSITY_ENABLE_70=n
CONFIG_SPARSITY_ENABLE_90=n
CONFIG_TEST_SPARSITY_50=y
CONFIG_PRUNING_WARMUP=100
CONFIG_PRUNING_FREQUENCY=50
CONFIG_PRUNING_LOG_SPARSITY=y

# Output Configuration
CONFIG_OUTPUT_DIR="bitter_lesson_results"
CONFIG_JSON_OUTPUT="training_metrics.json"
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=5000

# Tracking Configuration
CONFIG_TRACKER="wandb,trackio"

# Resource Configuration
CONFIG_NUM_WORKERS=16
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=2
CONFIG_COMPILE_MODEL=y
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_GRADIENT_CLIP_NORM="1.0"

# Verbose output for debugging
CONFIG_VERBOSE=y
CONFIG_LOG_LEVEL="INFO"
CONFIG_DEBUG=n
CONFIG_PROFILE=n

# Expected outcomes:
# 1. AdamWSPAM+magnitude: ~42 perplexity, 50% sparsity, baseline time
# 2. AdamWPrune bitter0: ~51 perplexity, 50% sparsity, -21% time
# 3. AdamWPrune bitter1: ~42 perplexity (expected), 50% sparsity, -21% time
# 4. AdamWPrune bitter2: Better than bitter1 (using +21% iterations), 50% sparsity
