# Example GPT-2 configuration with WandB experiment tracking
# Based on GPT-2 Shakespeare dataset for quick testing

CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration - Small model with Shakespeare for quick tests
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"
CONFIG_GPT2_DATASET_SHAKESPEARE=y
CONFIG_GPT2_DATASET_NAME="shakespeare"
CONFIG_GPT2_BLOCK_SIZE=256
CONFIG_GPT2_GRADIENT_ACCUMULATION=4
CONFIG_GPT2_EVAL_INTERVAL=50
CONFIG_GPT2_EVAL_SAMPLES=100
CONFIG_GPT2_WARMUP_STEPS=100
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=y
CONFIG_GPT2_COMPILE=y
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y

# Training parameters for quick testing
CONFIG_BATCH_SIZE=32
CONFIG_NUM_EPOCHS=1
CONFIG_LEARNING_RATE="6e-4"

# WandB experiment tracking configuration
CONFIG_EXPERIMENT_TRACKER="wandb"
CONFIG_TRACKER_PROJECT="adamwprune-gpt2-tests"
# CONFIG_TRACKER_RUN_NAME is not set - will be auto-generated
CONFIG_WANDB_OFFLINE=n  # Set to y for offline mode

# Optimizer configuration
CONFIG_OPTIMIZER_SELECT_ADAMW=y
CONFIG_OPTIMIZER_ADAMW=y
CONFIG_OPTIMIZER="adamw"
CONFIG_WEIGHT_DECAY="0.1"

# Training environment
CONFIG_NUM_WORKERS=4
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="data"
CONFIG_OUTPUT_DIR="outputs"

# Advanced options
CONFIG_COMPILE_MODEL=y
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_PIN_MEMORY=y
CONFIG_GPU_MONITOR=y
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1
