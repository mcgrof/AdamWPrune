#
# GPT-2 with MLA + Reciprocal MLP for AWS g5.12xlarge (4x NVIDIA A10G)
# Based on gpt2-finewebedu-a10gx4 with RA+MLA reciprocal mechanisms enabled
# Tests 6 ablation steps (0-5) with MLP reciprocity at ra_alpha=0.0
#

# Model Selection - GPT-2
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL="gpt2"
CONFIG_MODEL_GPT2=y
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_MODEL_NAME="gpt2"

# Dataset Configuration
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_DATASET="finewebedu"
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_DATA_DIR="./gpt2/data"

# Training Parameters - Matching A10x4 memory profile
CONFIG_BATCH_SIZE=2
CONFIG_GPT2_GRADIENT_ACCUMULATION=4
CONFIG_NUM_EPOCHS=10
CONFIG_GPT2_MAX_ITERS=10400
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_LOG_INTERVAL=10

# Learning Rate Schedule
CONFIG_LEARNING_RATE="6e-4"
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y
CONFIG_WEIGHT_DECAY="0.1"

# Optimizer - AdamWSPAM only (for consistency)
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWSPAM=y
CONFIG_OPTIMIZER="adamwspam"

# SPAM Configuration
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_PERIODIC_RESET=y
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP=y
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning - Disabled for RA+MLA experiments
CONFIG_PRUNING_MODE_NONE=y
CONFIG_PRUNING_METHOD="none"

# RA+MLA Configuration - MLP Reciprocity Focus
# ra_alpha=0.0 means NO reciprocal attention scoring overhead
# Focus is on the three reciprocal MLP mechanisms
CONFIG_ENABLE_RA_MLA=y
CONFIG_RA_MLA_LATENT_DIM=128
CONFIG_RA_MLA_RA_WINDOW=64
CONFIG_RA_MLA_RA_ALPHA="0.0"
CONFIG_RA_MLA_PER_HEAD_Q_LATENT=y
CONFIG_RA_MLA_PER_HEAD_V_UP=y
CONFIG_RA_MLA_USE_FLASH=n
CONFIG_RA_MLA_LOG_METRICS=y
CONFIG_RA_MLA_CACHE_Q_WINDOW=y
CONFIG_RA_MLA_USE_ROPE=n

# Reciprocal MLP Mechanisms (enabled via ablation steps)
CONFIG_RA_MLA_MLP_ATTN_GATE=y
CONFIG_RA_MLA_MLP_CROSS_TOKEN=y
CONFIG_RA_MLA_MLP_LATENT_RECIP=y

# Enable test matrix mode for ablation study
CONFIG_TEST_MATRIX_MODE=y
CONFIG_RA_MLA_ABLATION_STEPS="0,1,2,3,4,5"

# Multi-GPU Configuration for 4x A10G
# Note: For initial testing, may want single-GPU mode
# To enable DDP: uncomment CONFIG_GPT2_USE_DDP=y
CONFIG_GPT2_USE_DDP=y
CONFIG_GPT2_DDP_NUM_GPUS=4
CONFIG_GPT2_DDP_BACKEND="nccl"
CONFIG_GPT2_DDP_FIND_UNUSED_PARAMS=n

# Data Loading Optimization
CONFIG_NUM_WORKERS=12
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4

# Mixed Precision Training
CONFIG_MIXED_PRECISION=y
CONFIG_GPT2_AMP_DTYPE="bfloat16"

# Performance Optimizations
CONFIG_GPT2_CUDNN_BENCHMARK=y
CONFIG_GPT2_TF32_ALLOWED=y
CONFIG_GPU_WARMUP=y
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=n
CONFIG_GPT2_COMPILE=n
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y

# Memory Optimizations for Multi-GPU
CONFIG_PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# Checkpointing
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=1000

# Experiment Tracking
CONFIG_ENABLE_TRACKIO=y
CONFIG_ENABLE_WANDB=y
CONFIG_TRACKER_PROJECT="gpt2-mla-ra-mlp"

# Advanced options
CONFIG_COMPILE_MODEL=n
CONFIG_DEVICE="cuda"
