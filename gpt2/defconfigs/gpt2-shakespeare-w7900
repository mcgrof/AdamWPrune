# GPT-2 on Shakespeare dataset optimized for AMD W7900 (48GB)
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"

# GPT-2 Configuration - Use larger model for W7900
CONFIG_GPT2_MODEL_350M=y  # Can handle 350M model with 48GB
CONFIG_GPT2_MODEL_NAME="gpt2-medium"
CONFIG_GPT2_DATASET_SHAKESPEARE=y
CONFIG_GPT2_DATASET_NAME="shakespeare"
CONFIG_GPT2_BLOCK_SIZE=512  # Longer context for better learning
CONFIG_GPT2_GRADIENT_ACCUMULATION=8  # Higher accumulation for stable training
CONFIG_GPT2_EVAL_INTERVAL=100
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_WARMUP_STEPS=200  # More warmup for larger batch
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_DROPOUT="0.1"
CONFIG_GPT2_FLASH_ATTENTION=y
CONFIG_GPT2_COMPILE=y
CONFIG_GPT2_WEIGHT_TYING=y
CONFIG_GPT2_BIAS=y

# Training parameters optimized for W7900
CONFIG_BATCH_SIZE=128  # Large batch for 48GB VRAM
CONFIG_NUM_EPOCHS=3  # More epochs for better convergence
CONFIG_LEARNING_RATE="3e-4"  # Slightly lower LR for larger batch
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_NUM_WORKERS=8  # More workers for faster data loading
CONFIG_DEVICE="cuda"

# Optimizer - AdamW
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMW=y
CONFIG_OPTIMIZER="adamw"

# No pruning for baseline
CONFIG_PRUNING_MODE_NONE=y

# Advanced options
CONFIG_COMPILE_MODEL=y
CONFIG_MIXED_PRECISION=y  # Essential for large models
CONFIG_GPU_WARMUP=y
CONFIG_SAVE_CHECKPOINT=y
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4  # Higher prefetch for faster loading
