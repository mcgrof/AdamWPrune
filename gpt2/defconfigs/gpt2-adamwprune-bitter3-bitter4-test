# SPDX-License-Identifier: MIT
# Test AdamWPrune bitter3 and bitter4 variants
# Compares gradient-magnitude vs gradient-magnitude+layer-adaptive @ 50% sparsity

# Model Selection - Use GPT-2
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_MODE_MULTIPLE=n
CONFIG_MODEL_SELECT_GPT2=y
CONFIG_MODEL_SELECT_LENET5=n
CONFIG_MODEL_SELECT_RESNET18=n
CONFIG_MODEL_SELECT_RESNET50=n
CONFIG_MODEL_GPT2=y
CONFIG_MODEL="gpt2"
CONFIG_TEST_MODELS="gpt2"

# GPT-2 Model Configuration
CONFIG_GPT2_MODEL_124M=y
CONFIG_GPT2_DATASET_FINEWEBEDU=y
CONFIG_GPT2_BLOCK_SIZE=1024
CONFIG_GPT2_GRADIENT_ACCUMULATION=4
CONFIG_GPT2_EVAL_INTERVAL=500
CONFIG_GPT2_EVAL_SAMPLES=200
CONFIG_GPT2_WARMUP_STEPS=2000
CONFIG_GPT2_DECAY_LR=y
CONFIG_GPT2_MIN_LR="6e-5"
CONFIG_GPT2_FLASH_ATTENTION=y
CONFIG_GPT2_COMPILE=y

# Enable bitter3 and bitter4 variants (disable bitter0)
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER0=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER1=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER2=n
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER3=y
CONFIG_GPT2_ADAMWPRUNE_VARIANT_BITTER4=y

# Training Configuration
CONFIG_BATCH_SIZE=32
CONFIG_MAX_ITERS=10000  # Will be auto-adjusted to 13000 for bitter3/4
# GPT-2 uses iterations, not epochs - NUM_EPOCHS ignored
CONFIG_LEARNING_RATE="6e-4"
CONFIG_WEIGHT_DECAY="0.1"
CONFIG_DEVICE="cuda"
CONFIG_DTYPE="bfloat16"
CONFIG_LOG_INTERVAL=10

# Disable test matrix mode - we'll run directly
CONFIG_TEST_MATRIX_MODE=n

# Optimizer Configuration - Test AdamWPrune only
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWPRUNE=y
CONFIG_OPTIMIZER="adamwprune"

# AdamWPrune configuration
CONFIG_ADAMWPRUNE_BASE_ADAMWSPAM=y
CONFIG_ADAMWPRUNE_ENABLE_PRUNING=y
CONFIG_ADAMWPRUNE_TARGET_SPARSITY="0.5"
CONFIG_ADAMWPRUNE_BETA1="0.9"
CONFIG_ADAMWPRUNE_BETA2="0.95"
CONFIG_ADAMWPRUNE_WEIGHT_DECAY="0.1"
CONFIG_ADAMWPRUNE_WARMUP_STEPS=100
CONFIG_ADAMWPRUNE_FREQUENCY=50
CONFIG_ADAMWPRUNE_RAMP_END_STEP=13000  # Adjusted for bitter3/4
CONFIG_ADAMWPRUNE_VARIANT="bitter3"  # Default to bitter3

# Test bitter3 and bitter4 variants
CONFIG_TEST_ADAMWPRUNE_VARIANTS="bitter3,bitter4"

# SPAM settings (for base optimizer)
CONFIG_SPAM_ENABLE_CLIP=y
CONFIG_SPAM_SPIKE_THRESHOLD="2.0"
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_INTERVAL=1000
CONFIG_SPAM_WARMUP_STEPS=1000

# Pruning Configuration - Use state pruning only
CONFIG_PRUNING_MODE_SINGLE=y
CONFIG_PRUNING_SELECT_STATE=y
CONFIG_PRUNING_METHOD="state"

# Test only 50% sparsity
CONFIG_TARGET_SPARSITY="0.5"
CONFIG_SPARSITY_ENABLE_50=y
CONFIG_SPARSITY_ENABLE_70=n
CONFIG_SPARSITY_ENABLE_90=n
CONFIG_TEST_SPARSITY_50=y
CONFIG_PRUNING_WARMUP=100
CONFIG_PRUNING_FREQUENCY=50
CONFIG_PRUNING_LOG_SPARSITY=y

# Output Configuration
CONFIG_OUTPUT_DIR="bitter3_bitter4_comparison"
CONFIG_JSON_OUTPUT="comparison_metrics.json"
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=5000

# Tracking Configuration
CONFIG_TRACKER="wandb,trackio"

# Resource Configuration
CONFIG_NUM_WORKERS=16
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=2
CONFIG_COMPILE_MODEL=y
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_GRADIENT_CLIP_NORM="1.0"

# Verbose output for analysis
CONFIG_VERBOSE=y
CONFIG_LOG_LEVEL="INFO"
CONFIG_DEBUG=n
CONFIG_PROFILE=n

# Expected outcomes:
# bitter3: Gradient-magnitude pruning
#   - Importance: |w| * sqrt(|grad_avg|)
#   - Cubic sparsity schedule
#   - Uniform 50% sparsity across layers
#   - 13000 iterations (+30%)
#   - Target perplexity: ~42-44
#
# bitter4: Gradient-magnitude + layer-adaptive
#   - Importance: |w| * sqrt(|grad_avg|)
#   - Cubic sparsity schedule
#   - Layer-adaptive: 0.7x-1.3x base sparsity
#   - 13000 iterations (+30%)
#   - Target perplexity: ~40-42 (best expected)
