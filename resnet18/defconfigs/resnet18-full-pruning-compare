# SPDX-License-Identifier: MIT
# ResNet-18: Comprehensive pruning comparison across all optimizers
# Tests each optimizer with all supported pruning methods at standard sparsity levels
# Expected tests:
# - SGD, Adam, AdamW, AdamWAdv, AdamWSPAM: none + magnitude/movement @ 50/70/90% = 7 tests each
# - AdamWPrune: none + state @ 50/70/90% = 4 tests
# Total: 5*7 + 4 = 39 tests

# Enable multiple optimizer mode for comparison
CONFIG_OPTIMIZER_MODE_MULTIPLE=y

# ResNet-18 only configuration
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_RESNET18=y
CONFIG_RESNET18_NUM_CLASSES=10
CONFIG_RESNET18_DATASET="cifar10"
CONFIG_RESNET18_USE_AUGMENTATION=y
CONFIG_RESNET18_USE_COSINE_SCHEDULE=y
CONFIG_RESNET18_BASE_LR="0.1"
CONFIG_RESNET18_BATCH_SIZE=128
CONFIG_RESNET18_EPOCHS=100

# Training settings
CONFIG_BATCH_SIZE=128
CONFIG_NUM_EPOCHS=100
CONFIG_LEARNING_RATE="0.1"
CONFIG_NUM_WORKERS=8
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="data"
CONFIG_OUTPUT_DIR="results"
CONFIG_JSON_OUTPUT="training_metrics.json"

# Enable all optimizers for full comparison
CONFIG_OPTIMIZER_ENABLE_SGD=y
CONFIG_OPTIMIZER_ENABLE_ADAM=y
CONFIG_OPTIMIZER_ENABLE_ADAMW=y
CONFIG_OPTIMIZER_ENABLE_ADAMWADV=y
CONFIG_OPTIMIZER_ENABLE_ADAMWSPAM=y
CONFIG_OPTIMIZER_ENABLE_ADAMWPRUNE=y

# Pruning configuration:
# Enable all pruning methods that optimizers support
CONFIG_PRUNING_MODE_MULTIPLE=y
CONFIG_PRUNING_ENABLE_NONE=y       # Baseline for all optimizers
CONFIG_PRUNING_ENABLE_MAGNITUDE=y  # SGD, Adam, AdamW, AdamWAdv, AdamWSPAM
CONFIG_PRUNING_ENABLE_MOVEMENT=y   # SGD, Adam, AdamW, AdamWAdv, AdamWSPAM
CONFIG_PRUNING_ENABLE_STATE=y      # AdamWPrune only

# Standard sparsity levels for evaluation
CONFIG_SPARSITY_ENABLE_50=y
CONFIG_SPARSITY_ENABLE_70=y
CONFIG_SPARSITY_ENABLE_90=y
CONFIG_SPARSITY_ENABLE_95=n   # Skip extreme sparsity
CONFIG_SPARSITY_ENABLE_99=n   # Skip extreme sparsity

# Pruning parameters
CONFIG_PRUNING_WARMUP=1000
CONFIG_PRUNING_FREQUENCY=100
CONFIG_PRUNING_RAMP_END_EPOCH=75
CONFIG_PRUNING_LOG_SPARSITY=y

# Advanced settings
CONFIG_ADV_USE_AMSGRAD=n
CONFIG_ADV_WEIGHT_DECAY="0.0001"
CONFIG_ADV_USE_COSINE_ANNEALING=y
CONFIG_ADV_COSINE_ETA_MIN="1e-8"

# ResNet optimizations
CONFIG_COMPILE_MODEL=y
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4
CONFIG_GRADIENT_CLIP_NORM="1.0"
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=10

# Enable inference testing
CONFIG_INFERENCE_TEST=y
CONFIG_INFERENCE_BATCH_SIZES="1,16,32,64,128,256"

# GPU monitoring for both training and inference
CONFIG_GPU_MONITOR=y

# Logging
CONFIG_VERBOSE=y
CONFIG_LOG_LEVEL="INFO"
CONFIG_DEBUG=n
CONFIG_PROFILE=n

# Pruning analysis
CONFIG_PRUNING_SAVE_MASKS=y
CONFIG_PRUNING_VISUALIZE_MASKS=n

# SPAM settings (for AdamWSPAM)
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_INTERVAL=0
CONFIG_SPAM_WARMUP_STEPS=0
