# SPDX-License-Identifier: MIT
# ResNet-18: State pruning comparison across optimizers at 70% sparsity
# Compares AdamWPrune (built-in state) vs Adam, SGD with external state pruning

# Enable multiple optimizer mode for comparison
CONFIG_OPTIMIZER_MODE_MULTIPLE=y

# ResNet-18 only configuration
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_RESNET18=y
CONFIG_RESNET18_NUM_CLASSES=10
CONFIG_RESNET18_DATASET="cifar10"
CONFIG_RESNET18_USE_AUGMENTATION=y
CONFIG_RESNET18_USE_COSINE_SCHEDULE=y
CONFIG_RESNET18_BASE_LR="0.1"
CONFIG_RESNET18_BATCH_SIZE=128
CONFIG_RESNET18_EPOCHS=100

# Training settings
CONFIG_BATCH_SIZE=128
CONFIG_NUM_EPOCHS=100
CONFIG_LEARNING_RATE="0.1"
CONFIG_NUM_WORKERS=8
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="data"
CONFIG_OUTPUT_DIR="results"
CONFIG_JSON_OUTPUT="training_metrics.json"

# Enable optimizers for comparison (all 6)
CONFIG_OPTIMIZER_ENABLE_SGD=y
CONFIG_OPTIMIZER_ENABLE_ADAM=y
CONFIG_OPTIMIZER_ENABLE_ADAMW=y
CONFIG_OPTIMIZER_ENABLE_ADAMWADV=y
CONFIG_OPTIMIZER_ENABLE_ADAMWSPAM=y
CONFIG_OPTIMIZER_ENABLE_ADAMWPRUNE=y

# Pruning configuration:
# Test state pruning (AdamWPrune) vs movement pruning (others) at 70% sparsity
# - SGD, Adam, AdamW, AdamWAdv, AdamWSPAM: will use movement pruning
# - AdamWPrune: will use its built-in state pruning
CONFIG_PRUNING_MODE_MULTIPLE=y
CONFIG_PRUNING_ENABLE_NONE=n       # No baseline needed for this comparison
CONFIG_PRUNING_ENABLE_MAGNITUDE=n  # Not testing magnitude
CONFIG_PRUNING_ENABLE_MOVEMENT=y   # For SGD, Adam, etc.
CONFIG_PRUNING_ENABLE_STATE=y      # For AdamWPrune

# Only test 70% sparsity for this comparison
CONFIG_SPARSITY_ENABLE_50=n
CONFIG_SPARSITY_ENABLE_70=y
CONFIG_SPARSITY_ENABLE_90=n
CONFIG_TARGET_SPARSITY="0.7"
CONFIG_PRUNING_WARMUP=1000
CONFIG_PRUNING_FREQUENCY=100
CONFIG_PRUNING_RAMP_END_EPOCH=75
CONFIG_PRUNING_LOG_SPARSITY=y

# Advanced settings
CONFIG_ADV_USE_AMSGRAD=n
CONFIG_ADV_WEIGHT_DECAY="0.0001"
CONFIG_ADV_USE_COSINE_ANNEALING=y
CONFIG_ADV_COSINE_ETA_MIN="1e-8"

# ResNet optimizations
CONFIG_COMPILE_MODEL=y
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4
CONFIG_GRADIENT_CLIP_NORM="1.0"
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=10

# Enable inference testing
CONFIG_INFERENCE_TEST=y
CONFIG_INFERENCE_BATCH_SIZES="1,16,32,64,128,256"

# GPU monitoring for both training and inference
CONFIG_GPU_MONITOR=y

# Logging
CONFIG_VERBOSE=y
CONFIG_LOG_LEVEL="INFO"
CONFIG_DEBUG=n
CONFIG_PROFILE=n

# Pruning analysis
CONFIG_PRUNING_SAVE_MASKS=y
CONFIG_PRUNING_VISUALIZE_MASKS=n

# SPAM settings (for AdamWSPAM if enabled)
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_INTERVAL=0
CONFIG_SPAM_WARMUP_STEPS=0
