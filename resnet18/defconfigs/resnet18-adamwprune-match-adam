# SPDX-License-Identifier: MIT
# ResNet-18: AdamWPrune configured to match Adam optimizer performance
# Goal: Match resnet18_adam_none_0 accuracy (90.35%)

# Single optimizer mode for focused testing
CONFIG_OPTIMIZER_MODE_SINGLE=y
CONFIG_OPTIMIZER_SELECT_ADAMWPRUNE=y

# ResNet-18 configuration
CONFIG_MODEL_MODE_SINGLE=y
CONFIG_MODEL_SELECT_RESNET18=y
CONFIG_RESNET18_NUM_CLASSES=10
CONFIG_RESNET18_DATASET="cifar10"
CONFIG_RESNET18_USE_AUGMENTATION=y
CONFIG_RESNET18_USE_COSINE_SCHEDULE=y
CONFIG_RESNET18_BASE_LR="0.1"
CONFIG_RESNET18_BATCH_SIZE=128
CONFIG_RESNET18_EPOCHS=100

# Training settings
CONFIG_BATCH_SIZE=128
CONFIG_NUM_EPOCHS=100
CONFIG_LEARNING_RATE="0.1"
CONFIG_NUM_WORKERS=8
CONFIG_DEVICE="cuda"
CONFIG_DATA_DIR="data"
CONFIG_OUTPUT_DIR="results"
CONFIG_JSON_OUTPUT="training_metrics.json"

# Disable all pruning
CONFIG_PRUNING_MODE_NONE=y
CONFIG_ADAMWPRUNE_ENABLE_PRUNING=n

# Configure AdamWPrune to exactly match plain Adam
# Use Adam (not AdamW) as base optimizer
CONFIG_ADAMWPRUNE_BASE_ADAM=y

# AdamWPrune tuning to match Adam defaults
# - No weight decay (Adam default)
# - No AMSGrad (Adam default)
# - Keep beta1=0.9, beta2=0.999 (Adam defaults)
CONFIG_ADAMWPRUNE_BETA1=0.9
CONFIG_ADAMWPRUNE_BETA2=0.999
CONFIG_ADAMWPRUNE_WEIGHT_DECAY=0.0
CONFIG_ADAMWPRUNE_AMSGRAD=n

# Disable SPAM features to be more like vanilla Adam
CONFIG_SPAM_THETA="50.0"
CONFIG_SPAM_ENABLE_CLIP=n
CONFIG_SPAM_INTERVAL=0
CONFIG_SPAM_WARMUP_STEPS=0

# Disable advanced features to match basic Adam
CONFIG_ADV_USE_COSINE_ANNEALING=n
CONFIG_ADV_USE_AMSGRAD=n

# ResNet optimizations (keep these for fair comparison)
CONFIG_COMPILE_MODEL=y
CONFIG_MIXED_PRECISION=y
CONFIG_GPU_WARMUP=y
CONFIG_PIN_MEMORY=y
CONFIG_PERSISTENT_WORKERS=y
CONFIG_PREFETCH_FACTOR=4
CONFIG_SAVE_CHECKPOINT=y
CONFIG_CHECKPOINT_INTERVAL=10

# Enable inference testing
CONFIG_INFERENCE_TEST=n

# GPU monitoring
CONFIG_GPU_MONITOR=n

# Logging
CONFIG_VERBOSE=y
CONFIG_LOG_LEVEL="INFO"
CONFIG_DEBUG=n
CONFIG_PROFILE=n
