# SPDX-License-Identifier: MIT
# Reciprocal Attention (RA) + Multi-head Latent Attention (MLA) Configuration
# Experimental R&D feature - see docs/ra.md for details

menu "Reciprocal Attention (RA) + MLA (Experimental)"
	depends on MODEL_GPT2

config ENABLE_RA_MLA
	bool "Enable Reciprocal Attention + MLA (EXPERIMENTAL)"
	default n
	help
	  Enable experimental Reciprocal Attention with Multi-head Latent Attention.

	  This is a research R&D feature that reduces attention complexity from
	  O(n²·D) to O(n²·L) where L << D through low-rank KV compression.

	  Status: Implementation complete, validation in progress
	  See: docs/ra.md for full documentation and test roadmap

	  WARNING: This is experimental and unproven. Quality impact unknown.
	  Only use for research exploration, not production training.

if ENABLE_RA_MLA

config RA_MLA_LATENT_DIM
	int "Latent dimension for K/V compression"
	default 64
	range 16 512
	help
	  Latent dimension L for compressing keys and values.
	  Standard GPT-2 has D=64 per head, E=768 total.

	  Compression ratio = D / L:
	    L=32:  12× compression (high risk, max speed)
	    L=64:  6× compression (balanced, recommended)
	    L=128: 3× compression (safe, less benefit)
	    L=256: 1.5× compression (minimal risk, minimal benefit)

	  Lower L = faster but may hurt quality (UNTESTED!)
	  Higher L = safer but less speedup

	  Recommended: 64 for initial testing

config RA_MLA_RA_WINDOW
	int "Reciprocal attention window size"
	default 64
	range 16 512
	help
	  Width of local band for reciprocal symmetric attention.

	  Reciprocal scoring is applied within |i-j| <= W:
	    W=32:  Narrow local context
	    W=64:  Moderate local context (recommended)
	    W=128: Wide local context
	    W=256: Very wide context (expensive)

	  Larger W = more reciprocal computation but potentially better
	  Smaller W = faster but may miss long-range reciprocal patterns

	  Recommended: 64 for initial testing

config RA_MLA_RA_ALPHA
	string "Reciprocal attention weight (alpha)"
	default "0.5"
	help
	  Weight for the reciprocal symmetric term.

	  alpha=0.0: Pure MLA (no reciprocal, safest baseline)
	  alpha=0.5: Balanced reciprocal + standard (recommended)
	  alpha=1.0: Full reciprocal weight

	  Set to "0.0" to test pure MLA first before enabling RA.

	  Recommended: Start with "0.0" to validate MLA, then try "0.5"

config RA_MLA_PER_HEAD_Q_LATENT
	bool "Use per-head Q-to-latent projections"
	default y
	help
	  Whether to use separate Q-to-latent projections for each attention head.

	  Enabled (y): More expressive, more parameters
	    - Each head learns its own Q-to-latent mapping
	    - Better for complex tasks
	    - Parameters: H × D × L (e.g., 12 × 64 × 64 = 49K per layer)

	  Disabled (n): Shared projection across heads
	    - All heads use same Q-to-latent mapping
	    - Fewer parameters, faster
	    - Parameters: E × L (e.g., 768 × 64 = 49K per layer)

	  Recommended: Enable for maximum expressiveness

config RA_MLA_PER_HEAD_V_UP
	bool "Use per-head V up-projections"
	default y
	help
	  Whether to use separate V up-projections for each attention head.

	  Enabled (y): More expressive, more parameters
	    - Each head expands latent V to head space independently
	    - Parameters: H × L × D (e.g., 12 × 64 × 64 = 49K per layer)

	  Disabled (n): Shared V up-projection
	    - All heads use same expansion
	    - Fewer parameters, faster
	    - Parameters: L × D (e.g., 64 × 64 = 4K per layer)

	  Recommended: Enable for maximum expressiveness

config RA_MLA_USE_FLASH
	bool "Enable FlashAttention-style tiling"
	default n
	help
	  Use tiled implementation to reduce memory usage.

	  Enabled (y):
	    - Memory: O(n·L) instead of O(n²·H)
	    - 30-50% memory reduction expected
	    - Currently slower in pure PyTorch (no Triton kernel yet)
	    - No caching support yet (incompatible with generation)

	  Disabled (n):
	    - Standard implementation
	    - Full O(n²·H) memory for attention matrices
	    - Supports caching for generation
	    - Simpler, easier to debug

	  Recommended: Disable until Triton kernel is implemented
	  For training only, can enable to test memory savings.

config RA_MLA_LOG_METRICS
	bool "Log attention metrics (entropy, reciprocity)"
	default y
	help
	  Enable logging of attention-specific metrics:
	    - Attention entropy: measures distribution uniformity
	    - Reciprocity score: correlation between A[i,j] and A[j,i]

	  These metrics are useful for research analysis to understand
	  how RA+MLA differs from standard attention.

	  Minimal performance overhead (<1%).

	  Recommended: Enable for research experiments

config RA_MLA_CACHE_Q_WINDOW
	bool "Cache queries for reciprocal attention at inference"
	default y
	depends on !RA_MLA_USE_FLASH
	help
	  Cache a window of queries for computing reciprocal scores during
	  autoregressive generation.

	  Enabled (y):
	    - Stores last W queries (where W = RA_MLA_RA_WINDOW)
	    - Enables full RA during generation
	    - Memory overhead: O(W·H·D) per layer

	  Disabled (n):
	    - No query caching
	    - Reciprocal term only uses current step's query
	    - Less accurate but lower memory

	  Not available with FlashAttention tiling.

	  Recommended: Enable for generation quality

config RA_MLA_USE_ROPE
	bool "Use RoPE (Rotary Positional Embeddings)"
	default n
	help
	  Apply rotary positional embeddings to latent keys.

	  GPT-2 vanilla doesn't use RoPE (uses learned absolute positions).
	  Enable this only if you've retrofitted RoPE to your GPT-2 model.

	  Recommended: Disable (GPT-2 standard)

menu "Reciprocal MLP Mechanisms (Experimental)"
	depends on ENABLE_RA_MLA

comment "Reciprocal MLP extends RA+MLA by enabling bidirectional"
comment "information flow between attention and MLP layers."
comment "Based on scaling-inference.txt: shift compute from attention to MLP"

config RA_MLA_MLP_ATTN_GATE
	bool "Enable Mechanism 1: MLP-to-Attention Gating"
	default n
	help
	  MLP activations modulate attention head weights in the next layer.

	  How it works:
	    - MLP hidden states generate per-head gating weights
	    - These gates modulate attention output in the next layer
	    - Creates feedback loop where MLP influences attention focus

	  Benefits:
	    - MLP can guide attention based on learned features
	    - Enables cross-layer communication
	    - Low computational cost (small gating network)

	  Cost: +0.1-0.2% slowdown, minimal parameter increase

	  Ablation: Disable to test pure MLA+RA baseline
	  Recommended: Enable for full reciprocal MLP

config RA_MLA_MLP_GATE_ALPHA
	string "MLP gating mixing weight (alpha)"
	default "0.1"
	depends on RA_MLA_MLP_ATTN_GATE
	help
	  Weight for mixing gated attention: (1-α)*attn + α*(attn*gate)

	  α=0.0: No gating (disabled)
	  α=0.1: Light gating (recommended)
	  α=0.3: Moderate gating
	  α=0.5: Strong gating

	  Recommended: 0.1 for initial experiments

config RA_MLA_MLP_CROSS_TOKEN
	bool "Enable Mechanism 2: Cross-Token MLP Aggregation"
	default n
	help
	  MLP receives weighted sum of other tokens' activations using
	  attention weights for routing.

	  How it works:
	    - Reuses attention weights from attention layer
	    - Aggregates MLP hidden states across tokens
	    - Enables "attention mass" in MLP space at linear cost

	  Benefits:
	    - MLP gains cross-token context without O(n²) cost
	    - Complements attention's global view
	    - Improves long-range dependencies

	  Cost: +1-2% slowdown, one bmm per MLP layer

	  Ablation: Disable to test MLP without cross-token context
	  Recommended: Enable for full reciprocal MLP

config RA_MLA_MLP_CROSS_ALPHA
	string "Cross-token MLP mixing weight (alpha)"
	default "0.3"
	depends on RA_MLA_MLP_CROSS_TOKEN
	help
	  Weight for mixing cross-token context into MLP hidden state.

	  α=0.0: No cross-token aggregation (disabled)
	  α=0.3: Moderate aggregation (recommended)
	  α=0.5: Strong aggregation
	  α=0.7: Very strong aggregation

	  Recommended: 0.3 for initial experiments

config RA_MLA_MLP_LATENT_RECIP
	bool "Enable Mechanism 3: MLP Latent Space Reciprocity"
	default n
	help
	  Bidirectional information exchange between attention and MLP
	  latent spaces.

	  How it works:
	    - MLP projects to latent space (similar to attention)
	    - Attention latent → MLP (forward path)
	    - MLP latent → Attention (reciprocal path)
	    - Creates symmetric information flow

	  Benefits:
	    - Attention and MLP share latent representations
	    - Enables deeper architectural reciprocity
	    - Aligns with multi-latent compression principles

	  Cost: +2-3% slowdown, small latent projections

	  Ablation: Disable to test without latent-space coupling
	  Recommended: Enable for full reciprocal MLP

config RA_MLA_MLP_RECIP_ALPHA
	string "MLP latent reciprocity mixing weight (alpha)"
	default "0.2"
	depends on RA_MLA_MLP_LATENT_RECIP
	help
	  Weight for mixing latent contributions.

	  α=0.0: No latent reciprocity (disabled)
	  α=0.2: Light reciprocity (recommended)
	  α=0.4: Moderate reciprocity
	  α=0.6: Strong reciprocity

	  Recommended: 0.2 for initial experiments

config RA_MLA_MLP_LATENT_DIM
	int "MLP latent dimension"
	default 128
	range 64 256
	depends on RA_MLA_MLP_LATENT_RECIP
	help
	  Dimension of MLP latent space for mechanism 3.

	  Should be comparable to or larger than RA_MLA_LATENT_DIM
	  to avoid information bottleneck.

	  Values:
	    64: Minimal (matches typical RA_MLA_LATENT_DIM)
	    128: Recommended (2× attention latent)
	    256: Large (4× attention latent, more capacity)

	  Recommended: 128 (2× typical attention latent_dim=64)

config RA_MLA_MLP_GATE_DIM
	int "Gate context vector dimension"
	default 64
	range 32 128
	depends on RA_MLA_MLP_ATTN_GATE
	help
	  Dimension of intermediate context vector for gating network.

	  Smaller = fewer parameters, less expressive
	  Larger = more parameters, more expressive

	  Values:
	    32: Minimal gating capacity
	    64: Recommended
	    128: High gating capacity

	  Recommended: 64 for balanced performance

comment "Ablation Study Guidelines:"
comment "  Baseline (no reciprocal MLP): Disable all three mechanisms"
comment "  Test Mechanism 1 only: Enable MLP_ATTN_GATE"
comment "  Test Mechanisms 1+2: Enable MLP_ATTN_GATE + MLP_CROSS_TOKEN"
comment "  Full reciprocal MLP: Enable all three mechanisms"
comment ""
comment "Expected improvements (vs MLA-only baseline):"
comment "  Mechanism 1: -0.05 to -0.08 val_loss"
comment "  Mechanism 2: -0.10 to -0.15 val_loss (strongest)"
comment "  Mechanism 3: -0.05 to -0.10 val_loss"
comment "  All three: -0.20 to -0.30 val_loss (cumulative)"

endmenu # Reciprocal MLP Mechanisms

endif # ENABLE_RA_MLA

endmenu
