# SPDX-License-Identifier: MIT
# Reciprocal Attention (RA) + Multi-head Latent Attention (MLA) Configuration
# Experimental R&D feature - see docs/ra.md for details

menu "Reciprocal Attention (RA) + MLA (Experimental)"
	depends on MODEL_GPT2

config ENABLE_RA_MLA
	bool "Enable Reciprocal Attention + MLA (EXPERIMENTAL)"
	default n
	help
	  Enable experimental Reciprocal Attention with Multi-head Latent Attention.

	  This is a research R&D feature that reduces attention complexity from
	  O(n²·D) to O(n²·L) where L << D through low-rank KV compression.

	  Status: Implementation complete, validation in progress
	  See: docs/ra.md for full documentation and test roadmap

	  WARNING: This is experimental and unproven. Quality impact unknown.
	  Only use for research exploration, not production training.

if ENABLE_RA_MLA

config RA_MLA_LATENT_DIM
	int "Latent dimension for K/V compression"
	default 64
	range 16 512
	help
	  Latent dimension L for compressing keys and values.
	  Standard GPT-2 has D=64 per head, E=768 total.

	  Compression ratio = D / L:
	    L=32:  12× compression (high risk, max speed)
	    L=64:  6× compression (balanced, recommended)
	    L=128: 3× compression (safe, less benefit)
	    L=256: 1.5× compression (minimal risk, minimal benefit)

	  Lower L = faster but may hurt quality (UNTESTED!)
	  Higher L = safer but less speedup

	  Recommended: 64 for initial testing

config RA_MLA_RA_WINDOW
	int "Reciprocal attention window size"
	default 64
	range 16 512
	help
	  Width of local band for reciprocal symmetric attention.

	  Reciprocal scoring is applied within |i-j| <= W:
	    W=32:  Narrow local context
	    W=64:  Moderate local context (recommended)
	    W=128: Wide local context
	    W=256: Very wide context (expensive)

	  Larger W = more reciprocal computation but potentially better
	  Smaller W = faster but may miss long-range reciprocal patterns

	  Recommended: 64 for initial testing

config RA_MLA_RA_ALPHA
	string "Reciprocal attention weight (alpha)"
	default "0.5"
	help
	  Weight for the reciprocal symmetric term.

	  alpha=0.0: Pure MLA (no reciprocal, safest baseline)
	  alpha=0.5: Balanced reciprocal + standard (recommended)
	  alpha=1.0: Full reciprocal weight

	  Set to "0.0" to test pure MLA first before enabling RA.

	  Recommended: Start with "0.0" to validate MLA, then try "0.5"

config RA_MLA_PER_HEAD_Q_LATENT
	bool "Use per-head Q-to-latent projections"
	default y
	help
	  Whether to use separate Q-to-latent projections for each attention head.

	  Enabled (y): More expressive, more parameters
	    - Each head learns its own Q-to-latent mapping
	    - Better for complex tasks
	    - Parameters: H × D × L (e.g., 12 × 64 × 64 = 49K per layer)

	  Disabled (n): Shared projection across heads
	    - All heads use same Q-to-latent mapping
	    - Fewer parameters, faster
	    - Parameters: E × L (e.g., 768 × 64 = 49K per layer)

	  Recommended: Enable for maximum expressiveness

config RA_MLA_PER_HEAD_V_UP
	bool "Use per-head V up-projections"
	default y
	help
	  Whether to use separate V up-projections for each attention head.

	  Enabled (y): More expressive, more parameters
	    - Each head expands latent V to head space independently
	    - Parameters: H × L × D (e.g., 12 × 64 × 64 = 49K per layer)

	  Disabled (n): Shared V up-projection
	    - All heads use same expansion
	    - Fewer parameters, faster
	    - Parameters: L × D (e.g., 64 × 64 = 4K per layer)

	  Recommended: Enable for maximum expressiveness

config RA_MLA_USE_FLASH
	bool "Enable FlashAttention-style tiling"
	default n
	help
	  Use tiled implementation to reduce memory usage.

	  Enabled (y):
	    - Memory: O(n·L) instead of O(n²·H)
	    - 30-50% memory reduction expected
	    - Currently slower in pure PyTorch (no Triton kernel yet)
	    - No caching support yet (incompatible with generation)

	  Disabled (n):
	    - Standard implementation
	    - Full O(n²·H) memory for attention matrices
	    - Supports caching for generation
	    - Simpler, easier to debug

	  Recommended: Disable until Triton kernel is implemented
	  For training only, can enable to test memory savings.

config RA_MLA_LOG_METRICS
	bool "Log attention metrics (entropy, reciprocity)"
	default y
	help
	  Enable logging of attention-specific metrics:
	    - Attention entropy: measures distribution uniformity
	    - Reciprocity score: correlation between A[i,j] and A[j,i]

	  These metrics are useful for research analysis to understand
	  how RA+MLA differs from standard attention.

	  Minimal performance overhead (<1%).

	  Recommended: Enable for research experiments

config RA_MLA_CACHE_Q_WINDOW
	bool "Cache queries for reciprocal attention at inference"
	default y
	depends on !RA_MLA_USE_FLASH
	help
	  Cache a window of queries for computing reciprocal scores during
	  autoregressive generation.

	  Enabled (y):
	    - Stores last W queries (where W = RA_MLA_RA_WINDOW)
	    - Enables full RA during generation
	    - Memory overhead: O(W·H·D) per layer

	  Disabled (n):
	    - No query caching
	    - Reciprocal term only uses current step's query
	    - Less accurate but lower memory

	  Not available with FlashAttention tiling.

	  Recommended: Enable for generation quality

config RA_MLA_USE_ROPE
	bool "Use RoPE (Rotary Positional Embeddings)"
	default n
	help
	  Apply rotary positional embeddings to latent keys.

	  GPT-2 vanilla doesn't use RoPE (uses learned absolute positions).
	  Enable this only if you've retrofitted RoPE to your GPT-2 model.

	  Recommended: Disable (GPT-2 standard)

menu "Reciprocal MLP Mechanisms (Experimental)"
	depends on ENABLE_RA_MLA

comment "Reciprocal MLP extends RA+MLA by enabling bidirectional"
comment "information flow between attention and MLP layers."
comment "Based on scaling-inference.txt: shift compute from attention to MLP"

config RA_MLA_MLP_ATTN_GATE
	bool "Enable Mechanism 1: MLP-to-Attention Gating"
	default n
	help
	  MLP activations modulate attention head weights in the next layer.

	  How it works:
	    - MLP hidden states generate per-head gating weights
	    - These gates modulate attention output in the next layer
	    - Creates feedback loop where MLP influences attention focus

	  Benefits:
	    - MLP can guide attention based on learned features
	    - Enables cross-layer communication
	    - Low computational cost (small gating network)

	  Cost: +0.1-0.2% slowdown, minimal parameter increase

	  Ablation: Disable to test pure MLA+RA baseline
	  Recommended: Enable for full reciprocal MLP

config RA_MLA_MLP_GATE_ALPHA
	string "MLP gating mixing weight (alpha)"
	default "0.1"
	depends on RA_MLA_MLP_ATTN_GATE
	help
	  Weight for mixing gated attention: (1-α)*attn + α*(attn*gate)

	  α=0.0: No gating (disabled)
	  α=0.1: Light gating (recommended)
	  α=0.3: Moderate gating
	  α=0.5: Strong gating

	  Recommended: 0.1 for initial experiments

config RA_MLA_MLP_CROSS_TOKEN
	bool "Enable Mechanism 2: Cross-Token MLP Aggregation"
	default n
	help
	  MLP receives weighted sum of other tokens' activations using
	  attention weights for routing.

	  How it works:
	    - Reuses attention weights from attention layer
	    - Aggregates MLP hidden states across tokens
	    - Enables "attention mass" in MLP space at linear cost

	  Benefits:
	    - MLP gains cross-token context without O(n²) cost
	    - Complements attention's global view
	    - Improves long-range dependencies

	  Cost: +1-2% slowdown, one bmm per MLP layer

	  Ablation: Disable to test MLP without cross-token context
	  Recommended: Enable for full reciprocal MLP

config RA_MLA_MLP_CROSS_ALPHA
	string "Cross-token MLP mixing weight (alpha)"
	default "0.3"
	depends on RA_MLA_MLP_CROSS_TOKEN
	help
	  Weight for mixing cross-token context into MLP hidden state.

	  α=0.0: No cross-token aggregation (disabled)
	  α=0.3: Moderate aggregation (recommended)
	  α=0.5: Strong aggregation
	  α=0.7: Very strong aggregation

	  Recommended: 0.3 for initial experiments

config RA_MLA_MLP_LATENT_RECIP
	bool "Enable Mechanism 3: MLP Latent Space Reciprocity"
	default n
	help
	  Bidirectional information exchange between attention and MLP
	  latent spaces.

	  How it works:
	    - MLP projects to latent space (similar to attention)
	    - Attention latent → MLP (forward path)
	    - MLP latent → Attention (reciprocal path)
	    - Creates symmetric information flow

	  Benefits:
	    - Attention and MLP share latent representations
	    - Enables deeper architectural reciprocity
	    - Aligns with multi-latent compression principles

	  Cost: +2-3% slowdown, small latent projections

	  Ablation: Disable to test without latent-space coupling
	  Recommended: Enable for full reciprocal MLP

config RA_MLA_MLP_RECIP_ALPHA
	string "MLP latent reciprocity mixing weight (alpha)"
	default "0.2"
	depends on RA_MLA_MLP_LATENT_RECIP
	help
	  Weight for mixing latent contributions.

	  α=0.0: No latent reciprocity (disabled)
	  α=0.2: Light reciprocity (recommended)
	  α=0.4: Moderate reciprocity
	  α=0.6: Strong reciprocity

	  Recommended: 0.2 for initial experiments

config RA_MLA_MLP_LATENT_DIM
	int "MLP latent dimension"
	default 128
	range 64 256
	depends on RA_MLA_MLP_LATENT_RECIP
	help
	  Dimension of MLP latent space for mechanism 3.

	  Should be comparable to or larger than RA_MLA_LATENT_DIM
	  to avoid information bottleneck.

	  Values:
	    64: Minimal (matches typical RA_MLA_LATENT_DIM)
	    128: Recommended (2× attention latent)
	    256: Large (4× attention latent, more capacity)

	  Recommended: 128 (2× typical attention latent_dim=64)

config RA_MLA_MLP_GATE_DIM
	int "Gate context vector dimension"
	default 64
	range 32 128
	depends on RA_MLA_MLP_ATTN_GATE
	help
	  Dimension of intermediate context vector for gating network.

	  Smaller = fewer parameters, less expressive
	  Larger = more parameters, more expressive

	  Values:
	    32: Minimal gating capacity
	    64: Recommended
	    128: High gating capacity

	  Recommended: 64 for balanced performance

comment "Ablation Study Guidelines:"
comment "Baseline (no reciprocal MLP): Disable all three mechanisms"
comment "Test Mechanism 1 only: Enable MLP_ATTN_GATE"
comment "Test Mechanisms 1+2: Enable MLP_ATTN_GATE + MLP_CROSS_TOKEN"
comment "Full reciprocal MLP: Enable all three mechanisms"
comment ""
comment "Expected improvements (vs MLA-only baseline):"
comment "Mechanism 1: -0.05 to -0.08 val_loss"
comment "Mechanism 2: -0.10 to -0.15 val_loss (strongest)"
comment "Mechanism 3: -0.05 to -0.10 val_loss"
comment "All three: -0.20 to -0.30 val_loss (cumulative)"

endmenu # Reciprocal MLP Mechanisms

menu "Bitter-Scale Pruning (Inference-Optimized)"
	depends on ENABLE_RA_MLA

comment "Bitter-scale: Structure-aware head pruning targeting KV cache reduction"
comment "Directly operationalizes scaling-inference insights"
comment "Prune attention aggressively, expand MLP capacity"

config RA_MLA_BITTER_SCALE
	bool "Enable Bitter-Scale Pruning"
	default n
	help
	  Enable bitter-scale pruning: structure-aware attention head pruning
	  optimized for inference efficiency.

	  Design philosophy:
	    1. Favor MLP over attention at fixed params (scaling law)
	    2. Prune for KV bytes, not just parameter count
	    3. Multi-signal scoring: RA/MLA usage + attention mass + KV bytes
	    4. State-aligned pruning: properly clean up Adam optimizer states
	    5. Post-prune LR multipliers: lower attention, higher MLP

	  Why "bitter"? Leans into the bitter lesson: simple signals measured
	  relentlessly, with structure-aware pruning targeting IO/KV heavy parts.

	  This is a NEW bitter variant (Bitter 4+) that explicitly targets
	  inference efficiency rather than just parameter reduction.

	  Expected benefits:
	    - 4-8× smaller KV cache (combine MLA latent + head pruning)
	    - 15-20% faster inference despite wider MLPs
	    - Maintain or improve accuracy by shifting capacity to MLPs

	  Cost: +5-10% training overhead for signal collection

	  Recommended: Enable after validating MLA+Reciprocal MLP baseline

config RA_MLA_BITTER_PRUNE_INTERVAL
	int "Pruning interval (iterations)"
	default 1000
	range 100 5000
	depends on RA_MLA_BITTER_SCALE
	help
	  How often to evaluate and potentially prune heads (in iterations).

	  More frequent pruning:
	    - Responds faster to training dynamics
	    - Higher computational overhead
	    - May be too aggressive

	  Less frequent pruning:
	    - More stable, lets model adapt
	    - Lower overhead
	    - May miss optimal pruning opportunities

	  Recommended: 1000 iterations (balance stability and responsiveness)

config RA_MLA_BITTER_TARGET_HEADS
	int "Target number of heads to keep"
	default 8
	range 4 12
	depends on RA_MLA_BITTER_SCALE
	help
	  Target number of attention heads to keep after pruning.
	  GPT-2 starts with 12 heads.

	  Aggressive pruning (4-6 heads):
	    - Maximum KV cache reduction
	    - Requires strong reciprocal MLP to compensate
	    - Higher risk

	  Moderate pruning (7-9 heads):
	    - Good balance of efficiency and capacity
	    - Recommended starting point

	  Conservative pruning (10-11 heads):
	    - Minimal risk, modest gains
	    - Good for validation

	  Recommended: 8 heads (33% reduction, 1.5× KV cache compression)

config RA_MLA_BITTER_WEIGHT_RA
	string "Weight for RA/MLA usage signal"
	default "1.0"
	depends on RA_MLA_BITTER_SCALE
	help
	  Weight for RA/MLA usage score in multi-signal importance.

	  Score = w_ra*ra + w_attn*attn - w_kv*kv_bytes

	  Higher weight → prioritize heads with high RA/MLA participation
	  Lower weight → reduce influence of RA signals

	  Recommended: 1.0 (equal weight with attention mass)

config RA_MLA_BITTER_WEIGHT_ATTN
	string "Weight for attention mass signal"
	default "1.0"
	depends on RA_MLA_BITTER_SCALE
	help
	  Weight for attention mass (how much each head is used).

	  Attention mass measures aggregate attention probability per head.
	  High mass → head is actively routing information.

	  Recommended: 1.0 (equal weight with RA signals)

config RA_MLA_BITTER_WEIGHT_KVBYTES
	string "Weight for KV bytes penalty"
	default "0.5"
	depends on RA_MLA_BITTER_SCALE
	help
	  Weight for KV bytes in importance score (NEGATIVE term).

	  Score = w_ra*ra + w_attn*attn - w_kv*kv_bytes

	  Higher weight → more aggressive KV cache optimization
	  Lower weight → prioritize usage over KV savings

	  The negative term means: prune heads that save the most KV bytes
	  (if their usage doesn't justify the KV cost).

	  Recommended: 0.5 (balance usage and KV efficiency)

config RA_MLA_BITTER_LR_SCALE_ATTN
	string "Post-prune LR scale for attention"
	default "0.9"
	depends on RA_MLA_BITTER_SCALE
	help
	  Learning rate multiplier for attention parameters after pruning.

	  After pruning, remaining heads are high-value. Use conservative
	  LR to preserve what's important.

	  Values:
	    0.8: Very conservative (slow adaptation)
	    0.9: Recommended (stable but responsive)
	    1.0: No change (standard LR)

	  Recommended: 0.9 (slightly lower for stability)

config RA_MLA_BITTER_LR_SCALE_MLP
	string "Post-prune LR scale for MLP"
	default "1.1"
	depends on RA_MLA_BITTER_SCALE
	help
	  Learning rate multiplier for MLP parameters after pruning.

	  After pruning attention, encourage MLP to expand into freed capacity.
	  Higher LR helps MLP adapt faster.

	  Values:
	    1.0: No change (standard LR)
	    1.1: Recommended (gentle encouragement)
	    1.2: Aggressive (faster MLP growth)

	  Recommended: 1.1 (encourage MLP expansion)

config RA_MLA_BITTER_EMA_BETA
	string "EMA beta for signal smoothing"
	default "0.99"
	depends on RA_MLA_BITTER_SCALE
	help
	  Exponential moving average beta for smoothing importance signals.

	  Higher beta (closer to 1.0):
	    - Smoother, more stable signals
	    - Slower response to changes
	    - Less noise

	  Lower beta (closer to 0.9):
	    - Faster response to dynamics
	    - More noise, less stable
	    - May prune prematurely

	  Recommended: 0.99 (very smooth, stable pruning decisions)

comment "Integration with Reciprocal MLP:"
comment "Bitter-scale and reciprocal MLP share the same principle:"
comment "Compress attention aggressively, expand MLP capacity"
comment ""
comment "Expected combined effect:"
comment "MLA latent compression: 6× KV reduction"
comment "Bitter-scale head pruning: 1.5× KV reduction (12→8 heads)"
comment "Combined: 9× smaller KV cache"
comment "MLP expansion: 3072 → 4096-5120 hidden"
comment "Result: Faster inference + better accuracy"

endmenu # Bitter-Scale Pruning

menu "Reciprocal MLP Ablation Study (Multi-Run)"
	depends on ENABLE_RA_MLA

comment "Test multiple reciprocal MLP configurations in one run"
comment "Enables systematic ablation study of mechanisms 1, 2, and 3"

config RA_MLA_ABLATION_MODE
	bool "Enable Ablation Study Mode"
	default n
	help
	  Enable ablation study mode to run multiple reciprocal MLP
	  configurations in a single training session.

	  When enabled, runs 6 configurations:
	    0. Baseline: MLA-only (no RA, no reciprocal MLP)
	    1. Mechanism 1 only: MLP-to-Attention Gating
	    2. Mechanisms 1+2: + Cross-Token MLP Aggregation
	    3. Mechanisms 1+2+3: Full Reciprocal MLP
	    4. Mechanisms 1+2 (AdamWSPAM verification)
	    5. Full solution (all 3 mechanisms + AdamWSPAM)

	  Each configuration runs with the same baseline settings
	  (latent_dim=128, RA alpha=0.0) for fair comparison.

	  Results are saved separately for each configuration.

	  See: defconfigs/RA_MLA_RECIPROCAL_ABLATION_STUDY.md

config RA_MLA_ABLATION_BASELINE
	bool "Include baseline (MLA-only)"
	default y
	depends on RA_MLA_ABLATION_MODE
	help
	  Run baseline configuration (MLA-only, no reciprocal MLP).
	  Expected: val_loss ~3.61, ppl ~37.2

config RA_MLA_ABLATION_STEP1
	bool "Include step 1 (Mechanism 1 only)"
	default y
	depends on RA_MLA_ABLATION_MODE
	help
	  Run mechanism 1 only: MLP-to-Attention Gating.
	  Expected: -0.02 to -0.05 improvement over baseline.

config RA_MLA_ABLATION_STEP2
	bool "Include step 2 (Mechanisms 1+2)"
	default y
	depends on RA_MLA_ABLATION_MODE
	help
	  Run mechanisms 1+2: MLP gates + Cross-token MLP.
	  Expected: -0.05 to -0.10 improvement over baseline.
	  Predicted strongest contributor.

config RA_MLA_ABLATION_STEP3
	bool "Include step 3 (All mechanisms)"
	default y
	depends on RA_MLA_ABLATION_MODE
	help
	  Run all three mechanisms: Full reciprocal MLP.
	  Expected: -0.10 to -0.15 improvement over baseline.

config RA_MLA_ABLATION_STEP4
	bool "Include step 4 (Mechanisms 1+2, AdamWSPAM check)"
	default y
	depends on RA_MLA_ABLATION_MODE
	help
	  Run mechanisms 1+2 with AdamWSPAM (sanity check).
	  Expected: Should match step 2 results.

config RA_MLA_ABLATION_STEP5
	bool "Include step 5 (Full solution)"
	default y
	depends on RA_MLA_ABLATION_MODE
	help
	  Run full solution: All 3 mechanisms + AdamWSPAM.
	  Target: val_loss ~3.40-3.50 (45-48% better than vanilla).

config RA_MLA_ABLATION_STEPS
	string "Ablation steps to run"
	default "0,1,2,3,4,5" if RA_MLA_ABLATION_BASELINE && RA_MLA_ABLATION_STEP1 && RA_MLA_ABLATION_STEP2 && RA_MLA_ABLATION_STEP3 && RA_MLA_ABLATION_STEP4 && RA_MLA_ABLATION_STEP5
	default "1,2,3,4,5" if !RA_MLA_ABLATION_BASELINE && RA_MLA_ABLATION_STEP1 && RA_MLA_ABLATION_STEP2 && RA_MLA_ABLATION_STEP3 && RA_MLA_ABLATION_STEP4 && RA_MLA_ABLATION_STEP5
	default "0,1,2,3" if RA_MLA_ABLATION_BASELINE && RA_MLA_ABLATION_STEP1 && RA_MLA_ABLATION_STEP2 && RA_MLA_ABLATION_STEP3 && !RA_MLA_ABLATION_STEP4 && !RA_MLA_ABLATION_STEP5
	depends on RA_MLA_ABLATION_MODE
	help
	  Comma-separated list of ablation steps to run.
	  Format: "0,1,2,3,4,5"

	  0 = Baseline (MLA-only)
	  1 = Mechanism 1 only
	  2 = Mechanisms 1+2
	  3 = Mechanisms 1+2+3
	  4 = Mechanisms 1+2 (AdamWSPAM)
	  5 = Full solution (AdamWSPAM)

	  You can manually override this to run a subset, e.g., "0,2,5"

endmenu # Reciprocal MLP Ablation Study

endif # ENABLE_RA_MLA

endmenu
